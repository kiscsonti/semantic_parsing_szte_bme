{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import jsonlines\n",
    "file = open(\"Results.txt\",\"r\")\n",
    "scores=file.readlines()\n",
    "file.close()\n",
    "labellist=[]\n",
    "genrelist=[]\n",
    "sentencepairs=[]\n",
    "with jsonlines.open(\"multinli_1.0_dev_matched.jsonl\") as f:\n",
    "    for item in f:\n",
    "        labellist.append(item['gold_label'])\n",
    "        genrelist.append(item['genre'])\n",
    "        sentencepairs.append([item['sentence1'],item['sentence2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates an array for each possible treshold,\n",
    "#Then stores the evaluated results in a list in each slot\n",
    "\n",
    "from Evaluator import evaluate\n",
    "from Evaluator import evalrel\n",
    "\n",
    "evallist=evaluate(scores,labellist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "threshold=[x/1000 for x in range(1,999)]\n",
    "fig = plt.figure(figsize=(15, 10), dpi=100)\n",
    "plt.plot(threshold,evallist[0],label='accuracy',linewidth=2)\n",
    "plt.plot(threshold,evallist[1],label='Cont.Prec.',linewidth=2)\n",
    "plt.plot(threshold,evallist[2],label='Cont.Rec.',linewidth=2)\n",
    "plt.plot(threshold,evallist[3],label='Cont.Fscore',linewidth=2)\n",
    "plt.plot(threshold,evallist[4],label='Neut.Prec.',linewidth=2)\n",
    "plt.plot(threshold,evallist[5],label='Neut.Rec',linewidth=2)\n",
    "plt.plot(threshold,evallist[6],label='Neut.Fscore',linewidth=2)\n",
    "plt.plot(threshold,evallist[7],label='Ent.Prec',linewidth=2)\n",
    "plt.plot(threshold,evallist[8],label='Ent.Rec',linewidth=2)\n",
    "plt.plot(threshold,evallist[9],label='Ent.Fscore',linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Scores')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating based on just Neutral and not Neutral\n",
    "rellist=evalrel(scores,labellist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=[x/1000 for x in range(1,999)]\n",
    "fig2 = plt.figure(figsize=(15, 10), dpi=100)\n",
    "plt.plot(threshold,rellist[0],label='accuracy',linewidth=2)\n",
    "plt.plot(threshold,rellist[1],label='Rel.Prec.',linewidth=2)\n",
    "plt.plot(threshold,rellist[2],label='Rel.Rec.',linewidth=2)\n",
    "plt.plot(threshold,rellist[3],label='Rel.Fscore',linewidth=2)\n",
    "plt.plot(threshold,evallist[4],label='Neut.Prec.',linewidth=2)\n",
    "plt.plot(threshold,evallist[5],label='Neut.Rec',linewidth=2)\n",
    "plt.plot(threshold,evallist[6],label='Neut.Fscore',linewidth=2)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title('Scores')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers:\n",
    "for strange in evallist[10]:\n",
    "    print(sentencepairs[strange], genrelist[strange])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
