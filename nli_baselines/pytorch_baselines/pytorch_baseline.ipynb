{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from lstm_baseline import LSTMBaseline\n",
    "import torch\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parse the sentences from the jsonl file than split into a training set and a test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = []\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "with open('multinli_1.0_train.jsonl', 'r') as jsonl:\n",
    "    line = jsonl.readline()\n",
    "    while line is not None and line != \"\":\n",
    "        json_line = json.loads(line)\n",
    "        sentence1 = json_line['sentence1'].translate(translator).split()\n",
    "        sentence2 = json_line['sentence2'].translate(translator).split()\n",
    "        words1 = [word.lower() for word in sentence1 if word != \"-\"]\n",
    "        words2 = [word.lower() for word in sentence2 if word != \"-\"]\n",
    "        sentence_data.append((words1, words2, json_line['gold_label']))\n",
    "        line = jsonl.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = sentence_data[:500]\n",
    "test_data = sentence_data[500:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the dataset: 88022\n"
     ]
    }
   ],
   "source": [
    "tag_to_ix = {}\n",
    "words = []\n",
    "for sent1, sent2, tag in sentence_data:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "    words += sent1\n",
    "    words += sent2\n",
    "words = set(words)\n",
    "print(\"Number of unique words in the dataset:\", len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Test function to check the accuracy of the classification for each class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    classes = list(tag_to_ix.keys())\n",
    "    class_tp = list(0 for _ in range(len(classes)))\n",
    "    class_fp = list(0 for _ in range(len(classes)))\n",
    "    class_fn = list(0 for _ in range(len(classes)))\n",
    "    for sentence_hypothesis, sentence_premise, tag in test_data:\n",
    "        sentence_h = model.prepare_sequence(sentence_hypothesis)\n",
    "        sentence_p = model.prepare_sequence(sentence_premise)\n",
    "        tag = tag_to_ix[tag]\n",
    "        output = model(sentence_h, sentence_p).data\n",
    "        predicted = int((output == torch.max(output)).nonzero()[0])\n",
    "        if predicted == tag:\n",
    "            class_tp[tag] += 1\n",
    "        else:\n",
    "            class_fn[tag] += 1\n",
    "            class_fp[predicted] += 1\n",
    "    for i in range(3):\n",
    "        prec = class_tp[i] / (class_tp[i] + class_fp[i])\n",
    "        rec = class_tp[i] / (class_tp[i] + class_fn[i])\n",
    "        f1 = 2 / ((1/prec) + (1/rec)) if prec != 0 and rec != 0 else 0\n",
    "        print('F1 score of {0} : {1}, precision: {2}, recall: {3}'.format(classes[i], f1, prec, rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create an LSTMBaseline network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMBaseline(len(words), 300, 100, len(tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores before training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of contradiction : 0.450130548302872, precision: 0.2908232118758435, recall: 0.9953810623556582\nF1 score of entailment : 0, precision: 0.0, recall: 0.0\nF1 score of neutral : 0.0070052539404553416, precision: 0.13333333333333333, recall: 0.0035971223021582736\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores before training\")\n",
    "test(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.back_propagation(1, training_data, tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores after training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of entailment : 0.0375, precision: 0.20689655172413793, recall: 0.020618556701030927\nF1 score of neutral : 0.07633587786259542, precision: 0.3191489361702128, recall: 0.04335260115606936\nF1 score of contradiction : 0.5236985236985237, precision: 0.3647186147186147, recall: 0.928374655647383\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores after training\")\n",
    "test(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
